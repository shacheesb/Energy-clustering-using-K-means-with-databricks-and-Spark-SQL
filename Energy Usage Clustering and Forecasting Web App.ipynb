{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d50491f-4ba9-46d6-aa2d-9db39238661d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use legacy timestamp parser for Spark 3.x compatibility\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39bab3e1-1548-430b-88cf-de2ddb115b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+\n|      Date|    Time|Global_active_power|Global_reactive_power|Voltage|Global_intensity|Sub_metering_1|Sub_metering_2|Sub_metering_3|\n+----------+--------+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+\n|16/12/2006|17:24:00|              4.216|                0.418|234.840|          18.400|         0.000|         1.000|          17.0|\n|16/12/2006|17:25:00|              5.360|                0.436|233.630|          23.000|         0.000|         1.000|          16.0|\n|16/12/2006|17:26:00|              5.374|                0.498|233.290|          23.000|         0.000|         2.000|          17.0|\n|16/12/2006|17:27:00|              5.388|                0.502|233.740|          23.000|         0.000|         1.000|          17.0|\n|16/12/2006|17:28:00|              3.666|                0.528|235.680|          15.800|         0.000|         1.000|          17.0|\n+----------+--------+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True)\\\n",
    "    .option(\"sep\", \";\")\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .csv(\"/FileStore/tables/household_power_consumption.txt\")\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f89cd52-0863-4d43-b4bf-a49373fd96aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+-------------------+\n|Date      |Time    |Timestamp          |Global_active_power|\n+----------+--------+-------------------+-------------------+\n|16/12/2006|17:24:00|2006-12-16 17:24:00|4.216              |\n|16/12/2006|17:25:00|2006-12-16 17:25:00|5.36               |\n|16/12/2006|17:26:00|2006-12-16 17:26:00|5.374              |\n|16/12/2006|17:27:00|2006-12-16 17:27:00|5.388              |\n|16/12/2006|17:28:00|2006-12-16 17:28:00|3.666              |\n+----------+--------+-------------------+-------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, to_timestamp, col\n",
    "\n",
    "# Reload cleanly\n",
    "df = spark.read.option(\"header\", True)\\\n",
    "    .option(\"sep\", \";\")\\\n",
    "    .csv(\"/FileStore/tables/household_power_consumption.txt\")\n",
    "\n",
    "# Correctly parse timestamp and cast power usage\n",
    "df = df.withColumn(\"Timestamp\", to_timestamp(concat_ws(\" \", col(\"Date\"), col(\"Time\")), \"dd/MM/yyyy HH:mm:ss\")) \\\n",
    "       .withColumn(\"Global_active_power\", col(\"Global_active_power\").cast(\"double\")) \\\n",
    "       .dropna(subset=[\"Timestamp\", \"Global_active_power\"])\n",
    "\n",
    "# Check output\n",
    "df.select(\"Date\", \"Time\", \"Timestamp\", \"Global_active_power\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cffe43d-4671-40b3-91e6-92633b84b672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n|hour|          avg_power|\n+----+-------------------+\n|  12| 1.2070750260792096|\n|  22| 1.4126147300503793|\n|   1| 0.5393251638576786|\n|  13| 1.1445321537379893|\n|   6| 0.7915996434393993|\n|  16| 0.9489045760824987|\n|   3| 0.4448663201760571|\n|  20|  1.899064135957484|\n|   5|0.45367353976073366|\n|  19| 1.7333350634775906|\n|  15| 0.9907604535126148|\n|  17|  1.055109358520638|\n|   9| 1.3316453420225987|\n|   4| 0.4438472520130822|\n|   8| 1.4610164956914231|\n|  23| 0.9021416082281435|\n|   7| 1.5022463299094788|\n|  10| 1.2606274399399375|\n|  21| 1.8776974593412747|\n|  11| 1.2458220016891879|\n|  14| 1.0828393185410712|\n|   2| 0.4806205596975625|\n|   0| 0.6594341860546722|\n|  18| 1.3264510996285734|\n+----+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, avg\n",
    "\n",
    "hourly_usage = df.withColumn(\"hour\", hour(\"Timestamp\")) \\\n",
    "                 .groupBy(\"hour\") \\\n",
    "                 .agg(avg(\"Global_active_power\").alias(\"avg_power\"))\n",
    "\n",
    "hourly_usage.show(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff656129-65a8-495a-a283-10232ccc1f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+----------+\n|hour|          avg_power|prediction|\n+----+-------------------+----------+\n|   0| 0.6594341860546722|         1|\n|   1| 0.5393251638576786|         1|\n|   2| 0.4806205596975625|         1|\n|   3| 0.4448663201760571|         1|\n|   4| 0.4438472520130822|         1|\n|   5|0.45367353976073366|         1|\n|   6| 0.7915996434393993|         1|\n|   7| 1.5022463299094788|         0|\n|   8| 1.4610164956914231|         0|\n|   9| 1.3316453420225987|         0|\n|  10| 1.2606274399399375|         0|\n|  11| 1.2458220016891879|         0|\n|  12| 1.2070750260792096|         0|\n|  13| 1.1445321537379893|         0|\n|  14| 1.0828393185410712|         0|\n|  15| 0.9907604535126148|         0|\n|  16| 0.9489045760824987|         0|\n|  17|  1.055109358520638|         0|\n|  18| 1.3264510996285734|         0|\n|  19| 1.7333350634775906|         2|\n+----+-------------------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=[\"avg_power\"], outputCol=\"features\")\n",
    "features_df = vec_assembler.transform(hourly_usage)\n",
    "\n",
    "kmeans = KMeans(k=3, seed=42)\n",
    "model = kmeans.fit(features_df)\n",
    "clustered_df = model.transform(features_df)\n",
    "\n",
    "clustered_df.select(\"hour\", \"avg_power\", \"prediction\").orderBy(\"hour\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7046b46-3339-4c64-8959-50cd28155ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+----------+\n|hour|          avg_power|prediction|\n+----+-------------------+----------+\n|   0| 0.6594341860546722|         1|\n|   1| 0.5393251638576786|         1|\n|   2| 0.4806205596975625|         1|\n|   3| 0.4448663201760571|         1|\n|   4| 0.4438472520130822|         1|\n|   5|0.45367353976073366|         1|\n|   6| 0.7915996434393993|         1|\n|   7| 1.5022463299094788|         0|\n|   8| 1.4610164956914231|         0|\n|   9| 1.3316453420225987|         0|\n|  10| 1.2606274399399375|         0|\n|  11| 1.2458220016891879|         0|\n|  12| 1.2070750260792096|         0|\n|  13| 1.1445321537379893|         0|\n|  14| 1.0828393185410712|         0|\n|  15| 0.9907604535126148|         0|\n|  16| 0.9489045760824987|         0|\n|  17|  1.055109358520638|         0|\n|  18| 1.3264510996285734|         0|\n|  19| 1.7333350634775906|         2|\n+----+-------------------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=[\"avg_power\"], outputCol=\"features\")\n",
    "features_df = vec_assembler.transform(hourly_usage)\n",
    "\n",
    "kmeans = KMeans(k=3, seed=42)\n",
    "model = kmeans.fit(features_df)\n",
    "clustered_df = model.transform(features_df)\n",
    "\n",
    "clustered_df.select(\"hour\", \"avg_power\", \"prediction\").orderBy(\"hour\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053adabe-3415-491c-8fab-b45dfec04fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour,avg_power,prediction\n12,1.2070750260792096,0\n22,1.4126147300503793,0\n1,0.5393251638576786,1\n13,1.1445321537379893,0\n6,0.7915996434393993,1\n16,0.9489045760824987,0\n3,0.4448663201760571,1\n20,1.899064135957484,2\n5,0.45367353976073366,1\n19,1.7333350634775906,2\n15,0.9907604535126148,0\n17,1.055109358520638,0\n9,1.3316453420225987,0\n4,0.4438472520130822,1\n8,1.4610164956914231,0\n23,0.9021416082281435,0\n7,1.5022463299094788,0\n10,1.2606274399399375,0\n21,1.8776974593412747,2\n11,1.2458220016891879,0\n14,1.0828393185410712,0\n2,0.4806205596975625,1\n0,0.6594341860546722,1\n18,1.3264510996285734,0\n\n"
     ]
    }
   ],
   "source": [
    "with open(\"/dbfs/FileStore/hourly_clusters.csv\", \"r\") as f:\n",
    "    print(f.read())\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Energy Usage Clustering and Forecasting Web App",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}